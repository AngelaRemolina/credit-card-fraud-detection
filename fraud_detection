{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Card Fraud Detection\nAngela Remolina | Person code: 10992373","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:35.570650Z","iopub.execute_input":"2025-03-04T15:39:35.570943Z","iopub.status.idle":"2025-03-04T15:39:37.250274Z","shell.execute_reply.started":"2025-03-04T15:39:35.570922Z","shell.execute_reply":"2025-03-04T15:39:37.249575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîé Data Exploration","metadata":{}},{"cell_type":"code","source":"# Load data\n# data = pd.read_csv('creditcard.csv')\ndata = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:39.164157Z","iopub.execute_input":"2025-03-04T15:39:39.164624Z","iopub.status.idle":"2025-03-04T15:39:42.403960Z","shell.execute_reply.started":"2025-03-04T15:39:39.164593Z","shell.execute_reply":"2025-03-04T15:39:42.403101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see information about this dataset","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:42.405141Z","iopub.execute_input":"2025-03-04T15:39:42.405422Z","iopub.status.idle":"2025-03-04T15:39:42.453404Z","shell.execute_reply.started":"2025-03-04T15:39:42.405402Z","shell.execute_reply":"2025-03-04T15:39:42.452578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see the class count, where 0 = Regular transaction, 1 = Fraudulent transaction/","metadata":{}},{"cell_type":"code","source":"class_counts = data['Class'].value_counts()\nclass_labels = ['Non-Fraud', 'Fraud']\nclass_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:42.454819Z","iopub.execute_input":"2025-03-04T15:39:42.455182Z","iopub.status.idle":"2025-03-04T15:39:42.463946Z","shell.execute_reply.started":"2025-03-04T15:39:42.455160Z","shell.execute_reply":"2025-03-04T15:39:42.463157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have 284315 regular transactions and 492 fraudalent transactions, let's see this distribution of the data graphically","metadata":{}},{"cell_type":"code","source":"# Plot the bar plot for the number of frauds and non-frauds with count on top of the bars\ncolors = ['blue', 'red']\nsns.barplot(x=class_counts.index, y=class_counts.values, palette=colors)\n\n# Add count labels on top of the bars\nfor i, count in enumerate(class_counts.values):\n    percentage = count/len(data) * 100\n    plt.text(i, count, f\"{count} = {percentage:.2f}%\", ha='center')\n\n# Customize the plot\nplt.title('Number of Non-Fraud and Fraud Transactions', fontsize=14)\nplt.xlabel('Transaction Type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks([0, 1], class_labels, fontsize=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:42.906715Z","iopub.execute_input":"2025-03-04T15:39:42.906998Z","iopub.status.idle":"2025-03-04T15:39:43.215390Z","shell.execute_reply.started":"2025-03-04T15:39:42.906977Z","shell.execute_reply":"2025-03-04T15:39:43.214476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if data is normalized\ndata.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:43.216728Z","iopub.execute_input":"2025-03-04T15:39:43.217022Z","iopub.status.idle":"2025-03-04T15:39:43.589195Z","shell.execute_reply.started":"2025-03-04T15:39:43.216999Z","shell.execute_reply":"2025-03-04T15:39:43.588414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# violin plot for features\nplt.figure(figsize=(15, 10))\nsns.violinplot(data=data.drop(['Class'], axis=1), scale='width', inner='quartile', palette='muted')\nplt.title('Violin Plot of Features to Check Normalization', fontsize=14)\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Distribution', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:43.590171Z","iopub.execute_input":"2025-03-04T15:39:43.590496Z","iopub.status.idle":"2025-03-04T15:39:57.810244Z","shell.execute_reply.started":"2025-03-04T15:39:43.590473Z","shell.execute_reply":"2025-03-04T15:39:57.809402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üßπ Data preprocessing","metadata":{}},{"cell_type":"code","source":"X = data.drop(['Class'], axis=1)  # Features (Excluding 'Class')\ny = data['Class']                 # Target\n\n# Normalize features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Scale the entire feature set first\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:57.811540Z","iopub.execute_input":"2025-03-04T15:39:57.811896Z","iopub.status.idle":"2025-03-04T15:39:57.991429Z","shell.execute_reply.started":"2025-03-04T15:39:57.811864Z","shell.execute_reply":"2025-03-04T15:39:57.990586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# violin plot for features\nplt.figure(figsize=(15, 10))\nsns.violinplot(data=X_scaled, scale='width', inner='quartile', palette='muted')\nplt.title('Violin Plot of Features to Check Normalization', fontsize=14)\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Distribution', fontsize=12)\nplt.xticks(np.arange(30), X.columns, rotation=90)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:57.992567Z","iopub.execute_input":"2025-03-04T15:39:57.992808Z","iopub.status.idle":"2025-03-04T15:40:12.121946Z","shell.execute_reply.started":"2025-03-04T15:39:57.992787Z","shell.execute_reply":"2025-03-04T15:40:12.121010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:17.959006Z","iopub.execute_input":"2025-03-04T15:40:17.959353Z","iopub.status.idle":"2025-03-04T15:40:18.166264Z","shell.execute_reply.started":"2025-03-04T15:40:17.959325Z","shell.execute_reply":"2025-03-04T15:40:18.165358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To tackle the class umbalanced problem, an oversampling technique to augment the samples of class 1 (fraud)","metadata":{}},{"cell_type":"code","source":"# Apply SMOTE to the training set\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:20.747234Z","iopub.execute_input":"2025-03-04T15:40:20.747558Z","iopub.status.idle":"2025-03-04T15:40:20.998720Z","shell.execute_reply.started":"2025-03-04T15:40:20.747524Z","shell.execute_reply":"2025-03-04T15:40:20.998050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show the distribution of the target variable after applying SMOTE\nclass_counts_smote = y_train_smote.value_counts()\n\n# Plot the bar plot for the number of frauds and non-frauds with count on top of the bars\ncolors = ['blue', 'red']\nsns.barplot(x=class_counts_smote.index, y=class_counts_smote.values, palette=colors)\n\n# Add count labels on top of the bars\nfor i, count in enumerate(class_counts_smote.values):\n    percentage = count/len(X_train_smote) * 100\n    plt.text(i, count, f\"{count} = {percentage:.2f}%\", ha='center')\n\n# Customize the plot\nplt.title('Number of Non-Fraud and Fraud Transactions', fontsize=14)\nplt.xlabel('Transaction Type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks([0, 1], class_labels, fontsize=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:21.763647Z","iopub.execute_input":"2025-03-04T15:40:21.763931Z","iopub.status.idle":"2025-03-04T15:40:21.935546Z","shell.execute_reply.started":"2025-03-04T15:40:21.763910Z","shell.execute_reply":"2025-03-04T15:40:21.934704Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With this tecnique now both classes are balanced with a 50/50 distribution, now we can show the distribution of this sinthetic samples in the following scatter plot","metadata":{}},{"cell_type":"code","source":"# Original data scatter plot\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 1], X_train[:, 2], c=y_train, alpha=0.5, cmap='coolwarm')\nplt.title('Original Data')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\n# SMOTE-generated data scatter plot\nplt.subplot(1, 2, 2)\nplt.scatter(X_train_smote[:, 1], X_train_smote[:, 2], c=y_train_smote, alpha=0.5, cmap='coolwarm')\nplt.title('After SMOTE')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:24.396494Z","iopub.execute_input":"2025-03-04T15:40:24.396835Z","iopub.status.idle":"2025-03-04T15:40:35.819564Z","shell.execute_reply.started":"2025-03-04T15:40:24.396808Z","shell.execute_reply":"2025-03-04T15:40:35.818671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üó≥Ô∏è Feature selection","metadata":{}},{"cell_type":"markdown","source":"Before starting to try random models with all combination of features let's do an analysis on what features might fit better to this problem","metadata":{}},{"cell_type":"markdown","source":"### Correlation analysis","metadata":{}},{"cell_type":"code","source":"corr_matrix = pd.DataFrame(X_scaled, columns=data.drop(['Class'], axis=1).columns).corr()\nplt.figsize=(20,20)\nsns.heatmap(corr_matrix)\nplt.xticks(ticks=np.arange(len(corr_matrix.columns)), labels=corr_matrix.columns, rotation=90)\nplt.yticks(ticks=np.arange(len(corr_matrix.columns)), labels=corr_matrix.columns, rotation=0)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:35.820613Z","iopub.execute_input":"2025-03-04T15:40:35.820860Z","iopub.status.idle":"2025-03-04T15:40:36.838244Z","shell.execute_reply.started":"2025-03-04T15:40:35.820840Z","shell.execute_reply":"2025-03-04T15:40:36.837389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find corr_matrix values where it's absolute value is greater than 0.4 and print indices\nhigh_corr = corr_matrix[abs(corr_matrix) > 0.3].stack().dropna()\nhigh_corr = high_corr[high_corr.index.get_level_values(0) != high_corr.index.get_level_values(1)]\nhigh_corr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:47.202690Z","iopub.execute_input":"2025-03-04T15:40:47.202999Z","iopub.status.idle":"2025-03-04T15:40:47.222952Z","shell.execute_reply.started":"2025-03-04T15:40:47.202976Z","shell.execute_reply":"2025-03-04T15:40:47.222241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Importance from Tree-based Models","metadata":{}},{"cell_type":"code","source":"importances = RandomForestClassifier(random_state=42).fit(X_train_smote, y_train_smote).feature_importances_\nfeature_importance_df = pd.DataFrame({'Feature': data.drop(['Class'], axis=1).columns, \n                                      'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\nprint(feature_importance_df.head(10))  # Top 10 features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:40:49.741961Z","iopub.execute_input":"2025-03-04T15:40:49.742282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RFE method (Recursive Feature Elimination)","metadata":{}},{"cell_type":"code","source":"# Feature selection using Recursive Feature Elimination (RFE)\nestimator = LogisticRegression(random_state=42, max_iter=500)\nselector = RFE(estimator, n_features_to_select=10, step=1)\nselector.fit(X_train_smote, y_train_smote)\nselected_features = selector.support_\nselected_feature_names = data.drop(['Class'], axis=1).columns[selected_features]\nprint(\"Selected Features:\", selected_feature_names)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏èüß± Model building","metadata":{}},{"cell_type":"markdown","source":"We want to try different classifiers to compare results, and see which one fits best ","metadata":{}},{"cell_type":"code","source":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#todo, make another for that tries with different features\n#store the results of plot them right away with plt","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_selected = X_train_smote[:, selected_features]\nX_test_selected = X_test[:, selected_features]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key, classifier in classifiers.items():\n    classifier.fit(X_train_selected, y_train_smote)\n    training_score = cross_val_score(classifier, X_train_selected, y_train_smote, cv=5)\n    print(f\"Classifier: {key}\\nhas a training score of {round(training_score.mean(), 2) * 100} accuracy\")\n    print(\"\\nTesting Results:\")\n    # Evaluate on the test set\n    y_pred = classifier.predict(X_test_selected)\n    print(classification_report(y_test, y_pred))\n    print(f\"ROC-AUC Score: {roc_auc_score(y_test, classifier.predict_proba(X_test_selected)[:, 1]):.4f}\\n\")\n\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}